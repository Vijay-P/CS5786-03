{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import nltk # import Natural Language Toolkit\n",
    "nltk.download('wordnet') # download the corpus of words the NLTK library uses\n",
    "from nltk.stem import WordNetLemmatizer # import the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input the file name as a string\n",
    "# Outputs two lists: [listOfReviews], [listOfLabels]\n",
    "def loadAndParse(inputFileName):\n",
    "    # Open file\n",
    "    fIn = open(inputFileName)\n",
    "\n",
    "    # split the the file into lines\n",
    "    lines = fIn.read().splitlines()\n",
    "\n",
    "    # Now split each line on tabs to get text and label\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in lines:\n",
    "        messageAndLabelList = review.split('\\t')\n",
    "        message = messageAndLabelList[0]\n",
    "        label = messageAndLabelList[1]\n",
    "        reviews.append(message)\n",
    "        labels.append(label)\n",
    "    return reviews, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get three lists of reviews and three lists of labels\n",
    "yelpReviews, yelpLabels = loadAndParse('yelp_labelled.txt')\n",
    "imdbReviews, imdbLabels = loadAndParse('imdb_labelled.txt')\n",
    "amazonReviews, amazonLabels = loadAndParse('amazon_cells_labelled.txt')\n",
    "\n",
    "# Make two big lists: one of all reviews and one of all labels, in matching order.\n",
    "allReviews = []\n",
    "allLabels = []\n",
    "allReviews = yelpReviews + imdbReviews + amazonReviews\n",
    "allLabels = yelpLabels + imdbLabels + amazonLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1500 positive reviews.\n",
      "There are 1500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "positives , negatives = 0, 0\n",
    "for label in allLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        positives += 1\n",
    "    if label == 0:\n",
    "        negatives += 1\n",
    "    \n",
    "print(\"There are\", positives, \"positive reviews.\")\n",
    "print(\"There are\", negatives, \"negative reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working punctuation remover. It can do whole sentences.\n",
    "def stripPunctuation(input):\n",
    "    translation_table = dict.fromkeys(map(ord, '$#%&!()*+,-./:;<=>?@[\\]^_`{|}~'), None)\n",
    "    output = input.translate(translation_table)\n",
    "    # from: https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working word lemmatizer. It works on single words.\n",
    "def lemmatizeWord(input):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    output = lemmatize.lemmatize(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEPS\n",
    "# 1) import each of the three files\n",
    "# 2) create a list of strings from each file, where each string is a message\n",
    "# 3) Clean each string: strip punctuation\n",
    "\n",
    "# 3b) Create training set and test set\n",
    "\n",
    "# 4) split each string into a list of individual words by splitting on spaces\n",
    "# 5) add the lists to make one big list of words \n",
    "# 6) lowercase and lemmatize every word in the list\n",
    "# 7) convert the list into a set to get rid of repeats. This set is the corpus.\n",
    "# 8) Maybe convert the set back into a list if that's needed to iterate over the list\n",
    "\n",
    "# ... remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
