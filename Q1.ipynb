{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import nltk # import Natural Language Toolkit\n",
    "nltk.download('wordnet') # download the corpus of words the NLTK library uses\n",
    "from nltk.stem import WordNetLemmatizer # import the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input the file name as a string\n",
    "# Outputs two lists: [listOfReviews], [listOfLabels]\n",
    "def loadAndParse(inputFileName):\n",
    "    # Open file\n",
    "    fIn = open(inputFileName)\n",
    "\n",
    "    # split the the file into lines\n",
    "    lines = fIn.read().splitlines()\n",
    "\n",
    "    # Now split each line on tabs to get text and label\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in lines:\n",
    "        messageAndLabelList = review.split('\\t')\n",
    "        if(len(messageAndLabelList) != 2):\n",
    "            print(review)\n",
    "        message = messageAndLabelList[0]\n",
    "        label = messageAndLabelList[1]\n",
    "        reviews.append(message)\n",
    "        labels.append(label)\n",
    "    return reviews, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get three lists of reviews and three lists of labels\n",
    "yelpReviews, yelpLabels = loadAndParse('sentiment_labelled_sentences/yelp_labelled.txt')\n",
    "imdbReviews, imdbLabels = loadAndParse('sentiment_labelled_sentences/imdb_labelled.txt')\n",
    "amazonReviews, amazonLabels = loadAndParse('sentiment_labelled_sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "# Make two big lists: one of all reviews and one of all labels, in matching order.\n",
    "allReviews = []\n",
    "allLabels = []\n",
    "allReviews = yelpReviews + imdbReviews + amazonReviews\n",
    "allLabels = yelpLabels + imdbLabels + amazonLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON: There are 500 positive reviews.\n",
      "AMAZON: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "amazon_positives , amazon_negatives = 0, 0\n",
    "for label in amazonLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        amazon_positives += 1\n",
    "    if label == 0:\n",
    "        amazon_negatives += 1\n",
    "    \n",
    "print(\"AMAZON: There are\", amazon_positives, \"positive reviews.\")\n",
    "print(\"AMAZON: There are\", amazon_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB: There are 500 positive reviews.\n",
      "IMDB: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "imdb_positives , imdb_negatives = 0, 0\n",
    "for label in imdbLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        imdb_positives += 1\n",
    "    if label == 0:\n",
    "        imdb_negatives += 1\n",
    "    \n",
    "print(\"IMDB: There are\", imdb_positives, \"positive reviews.\")\n",
    "print(\"IMDB: There are\", imdb_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YELP: There are 500 positive reviews.\n",
      "YELP: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "yelp_positives , yelp_negatives = 0, 0\n",
    "for label in yelpLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        yelp_positives += 1\n",
    "    if label == 0:\n",
    "        yelp_negatives += 1\n",
    "    \n",
    "print(\"YELP: There are\", yelp_positives, \"positive reviews.\")\n",
    "print(\"YELP: There are\", yelp_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working punctuation remover. It can do whole sentences.\n",
    "def stripPunctuation(input):\n",
    "    translation_table = dict.fromkeys(map(ord, '\"\\'1234567890$#%&!()*+,-./:;<=>?@[\\]^_`{|}~'), None)\n",
    "    output = input.translate(translation_table)\n",
    "    # from: https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working word lemmatizer. It works on single words.\n",
    "def lemmatizeWord(input):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    output = lemmatize.lemmatize(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation and stop words from a sentence.\n",
    "def cleanAndRemoveStopWords(input):\n",
    "    stopWords = ['i', 'if', 'what','who','is','a','an','and', 'at','are','as','be','by','for', 'he', 'in', 'it', 'its', 'of', 'on', 'or', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n",
    "    queryWords = stripPunctuation(input).split()\n",
    "    resultWords = [lemmatizeWord(word) for word in queryWords if lemmatizeWord(word.lower()) not in stopWords]\n",
    "    \n",
    "    resultWordsLower = []\n",
    "    for word in resultWords:\n",
    "        resultWordsLower.append(word.lower())\n",
    "    \n",
    "    return resultWordsLower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.) Split the training set and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training and test sets for each of the three websites\n",
    "amazonReviewsTrain = amazonReviews[0:400]\n",
    "amazonLabelsTrain = amazonLabels[0:400]\n",
    "amazonReviewsTest = amazonReviews[400: 500]\n",
    "amazonLabelsTest = amazonLabels[400:500]\n",
    "\n",
    "imdbReviewsTrain = imdbReviews[0:400]\n",
    "imdbLabelsTrain = imdbLabels[0:400]\n",
    "imdbReviewsTest = imdbReviews[400: 500]\n",
    "imdbLabelsTest = imdbLabels[400:500]\n",
    "\n",
    "yelpReviewsTrain = yelpReviews[0:400]\n",
    "yelpLabelsTrain = yelpLabels[0:400]\n",
    "yelpReviewsTest = yelpReviews[400: 500]\n",
    "yelpLabelsTest = yelpLabels[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d.) Bag of words model.\n",
    "Extract features and then represent each review using bag of words\n",
    "model, i.e., every word in the review becomes its own element in a feature vector. In order to\n",
    "do this, first, make one pass through all the reviews in the training set (**Explain why** we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. Then,make another pass\n",
    "through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. The i th element of a review’s feature vector is the number of\n",
    "occurrences of the i th dictionary word in the review. Implement the bag of words model and\n",
    "report feature vectors of any two reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make lists of all training reviews and labels.\n",
    "allReviewsTrain = amazonReviewsTrain + imdbReviewsTrain + yelpReviewsTrain\n",
    "allLabelsTrain = amazonLabelsTrain + imdbLabelsTrain + yelpLabelsTrain\n",
    "\n",
    "allReviewsTest = amazonReviewsTest + imdbReviewsTest + yelpReviewsTest\n",
    "allLabelsTest = amazonLabelsTest + imdbLabelsTest + yelpLabelsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that returns a non-unique list of all non stop-words.\n",
    "# Input: test set or training set of reviews.\n",
    "\n",
    "def cleanReviews(input):\n",
    "    result = []\n",
    "    for review in input:\n",
    "        cleanReview = cleanAndRemoveStopWords(review)\n",
    "        for cleanWord in cleanReview:\n",
    "            result.append(cleanWord)\n",
    "    return result\n",
    "\n",
    "allWordsNotUnique = cleanReviews(allReviewsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that makes a list of unique words (corpus), and a parallel list of counts \n",
    "# of each unique word. As input, it takes a list of non-unique words.\n",
    "def makeListsOfUniquesAndCounts(input):\n",
    "    uniques = []\n",
    "    for word in input:\n",
    "        if word not in uniques:\n",
    "            uniques.append(word)\n",
    "    # Sort the list of uniques\n",
    "    uniques.sort()\n",
    "    \n",
    "    #now \"uniques\" is an alphabetical list of all unique words in the corpus.\n",
    "    counts = [0]*len(uniques) # create a list of zeros as long as the number of unique words.\n",
    "    for word in input:\n",
    "        index = uniques.index(word)\n",
    "        counts[index] += 1 \n",
    "    return uniques, counts\n",
    "\n",
    "# Call the function to define the corpus.\n",
    "corpus, counts = makeListsOfUniquesAndCounts(allWordsNotUnique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each review against the corpus (from the training set). This defines each feature vector.\n",
    "corpusSet = set(corpus)\n",
    "def populateFeatureVectors(corpus, inputReviewsList):\n",
    "    # feature vectors are row vectors, each column is a feature representing one word from the corpus.\n",
    "    vectorsList = []\n",
    "    for review in inputReviewsList:\n",
    "        cleanedReview = cleanAndRemoveStopWords(review)\n",
    "        featureVector = [0]*len(corpus)\n",
    "        for word in cleanedReview:\n",
    "            if word in corpusSet:\n",
    "                index = corpus.index(word)\n",
    "                featureVector[index] += 1 \n",
    "        vectorsList.append(featureVector)\n",
    "    return vectorsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the training data.\n",
    "vectorsList = populateFeatureVectors(corpus, allReviewsTrain)\n",
    "# Make the list into a numpy array.\n",
    "vectorsArray = np.array(vectorsList)\n",
    "# Normalize each (now cleaned) row using the log norm, because it minimizes the variance of resulting vector.\n",
    "normedTrain = np.log10(vectorsArray + 1)\n",
    "\n",
    "\n",
    "# Clean the test data.\n",
    "cleanedTestReviews = populateFeatureVectors(corpus, allReviewsTest)\n",
    "# Make the list into a numpy array.\n",
    "testArray = np.array(cleanedTestReviews)\n",
    "# Norm the test data.\n",
    "normedTest = np.log10(testArray + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:      114       34\n",
      "Actual. F:       40      112\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T   Pred. F\n",
      "Actual. T:  0.380000  0.113333\n",
      "Actual. F:  0.133333  0.373333\n"
     ]
    }
   ],
   "source": [
    "# Use a logistic regression model to predict the labels of the list of test reviews.\n",
    "\n",
    "# Train the model.\n",
    "sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_pred = sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain).predict(normedTest)\n",
    "# Print some results from the logistic regression to see how it does (it does very badly).\n",
    "    # print(\"Predicted labels: \", list(sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain).predict(normedTest[:15])) )\n",
    "    # print(\"Actual labels:    \", allLabels[:15])\n",
    "# Make a confusion matrix of the logistic regression results\n",
    "y_true = allLabelsTest\n",
    "logisticConfusionMtx = sk.metrics.confusion_matrix(y_true, y_pred)\n",
    "logisticConfusionMtxDF = pd.DataFrame(np.array(logisticConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:       87       61\n",
      "Actual. F:       26      126\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T   Pred. F\n",
      "Actual. T:  0.290000  0.203333\n",
      "Actual. F:  0.086667  0.420000\n"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes to predict the labels of the list of test reviews.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model.\n",
    "y_predNB = GaussianNB().fit(normedTrain, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_predNB = GaussianNB().fit(normedTrain, allLabelsTrain).predict(normedTest)\n",
    "# Make a confusion matrix of the Naive Bayes prediction results\n",
    "y_true = allLabelsTest\n",
    "NBConfusionMtx = sk.metrics.confusion_matrix(y_true, y_predNB)\n",
    "NBConfusionMtxDF = pd.DataFrame(np.array(NBConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"NAIVE BAYES\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(NBConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(NBConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d.) n-grams model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams model\n",
    "\n",
    "# 0) make a list of cleaned reviews\n",
    "     # input: list of reviews\n",
    "     # output: list of cleaned reviews\n",
    "def getCleanListOfReviews(input):\n",
    "    cleanReviewsList = []\n",
    "    for review in input:\n",
    "        cleanReview = cleanAndRemoveStopWords(review)\n",
    "        cleanReviewsList.append(cleanReview)\n",
    "    return cleanReviewsList\n",
    "\n",
    "cleanRevsTrain = getCleanListOfReviews(allReviewsTrain)\n",
    "cleanRevsTest = getCleanListOfReviews(allReviewsTest)\n",
    "\n",
    "\n",
    "# 1) create a new corpus of n-grams from the training set\n",
    "def composeNGramsCorpus(cleanRevsList):\n",
    "    twoGrams = []\n",
    "    for review in cleanRevsList:\n",
    "        length = len(review)\n",
    "#         print(review) #for testing\n",
    "#         print(\"There are _ words in the cleaned review:\", length) #for testing\n",
    "        for word in review:\n",
    "            index = review.index(word)\n",
    "            if index != length-1: #minus one because there is already one fewer 2-grams than words.\n",
    "                twoGram = word + \" \" + review[index + 1]\n",
    "#                 print(twoGram) #for testing\n",
    "                if twoGram not in twoGrams:\n",
    "                    twoGrams.append(twoGram)\n",
    "#     print(len(twoGrams)) #for testing          \n",
    "    return twoGrams\n",
    "\n",
    "nGramsCorpus = composeNGramsCorpus(cleanRevsTrain)\n",
    "\n",
    "\n",
    "# 1b) Get each review in the form of a list of n-grams.\n",
    "def getReviewsAsNGrams(cleanRevsList):\n",
    "    listOfReviewsAsNGrams = []\n",
    "    for review in cleanRevsList:\n",
    "        length = len(review)\n",
    "        twoGrams = []\n",
    "        for word in review:\n",
    "            index = review.index(word)\n",
    "            if index != length-1: #minus one because there is already one fewer 2-grams than words.\n",
    "                twoGram = word + \" \" + review[index + 1]\n",
    "                twoGrams.append(twoGram)\n",
    "        listOfReviewsAsNGrams.append(twoGrams)\n",
    "    return listOfReviewsAsNGrams\n",
    "\n",
    "trainRevsAsNGrams = getReviewsAsNGrams(cleanRevsTrain)\n",
    "testRevsAsNGrams = getReviewsAsNGrams(cleanRevsTest)\n",
    "            \n",
    "# 2) populate the feature vectors of each review using 2-grams\n",
    "def populateTwoGramsVectors(nGramsCorpus, revsAsNGramsListOfLists):\n",
    "    nGramsCorpusSet = set(nGramsCorpus)\n",
    "    # feature vectors = row vectors, each column = unique n-gram from the n-grams corpus.\n",
    "    vectorsList = []\n",
    "    for review in revsAsNGramsListOfLists:\n",
    "        featureVector = [0]*len(nGramsCorpus)\n",
    "        for nGram in review:     \n",
    "            if nGram in nGramsCorpusSet:\n",
    "                index = nGramsCorpus.index(nGram)\n",
    "                featureVector[index] += 1\n",
    "        vectorsList.append(featureVector)\n",
    "    return vectorsList\n",
    "\n",
    "nGramsVectorsTrain = populateTwoGramsVectors(nGramsCorpus, trainRevsAsNGrams)\n",
    "nGramsVectorsTest = populateTwoGramsVectors(nGramsCorpus, testRevsAsNGrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Norm the training data.\n",
    "vectorsArrayNG = np.array(nGramsVectorsTrain)\n",
    "# Normalize each (now cleaned) row using the log norm, because it minimizes the variance of resulting vector.\n",
    "normedTrainNG = np.log10(vectorsArrayNG + 1)\n",
    "\n",
    "# Norm the test data\n",
    "testArrayNG = np.array(nGramsVectorsTest)\n",
    "# Norm the test data.\n",
    "normedTestNG = np.log10(testArrayNG + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:       34      114\n",
      "Actual. F:       20      132\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T  Pred. F\n",
      "Actual. T:  0.113333     0.38\n",
      "Actual. F:  0.066667     0.44\n"
     ]
    }
   ],
   "source": [
    "# Use a logistic regression model to predict the labels of the list of test reviews.\n",
    "\n",
    "# Train the model.\n",
    "sk.linear_model.LogisticRegression().fit(normedTrainNG, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_predNG = sk.linear_model.LogisticRegression().fit(normedTrainNG, allLabelsTrain).predict(normedTestNG)\n",
    "# Make a confusion matrix of the logistic regression results\n",
    "y_true = allLabelsTest\n",
    "logisticConfusionMtx = sk.metrics.confusion_matrix(y_true, y_predNG)\n",
    "logisticConfusionMtxDF = pd.DataFrame(np.array(logisticConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-Grams with Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:       53       95\n",
      "Actual. F:       16      136\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T   Pred. F\n",
      "Actual. T:  0.176667  0.316667\n",
      "Actual. F:  0.053333  0.453333\n"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes to predict the labels of the list of test reviews.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model.\n",
    "y_predNB_NG = GaussianNB().fit(normedTrainNG, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_predNB_NG = GaussianNB().fit(normedTrainNG, allLabelsTrain).predict(normedTestNG)\n",
    "# Make a confusion matrix of the Naive Bayes prediction results\n",
    "y_true = allLabelsTest\n",
    "NBConfusionMtx = sk.metrics.confusion_matrix(y_true, y_predNB_NG)\n",
    "NBConfusionMtxDF = pd.DataFrame(np.array(NBConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"NAIVE BAYES\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(NBConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(NBConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2641, 2641)\n"
     ]
    }
   ],
   "source": [
    "# 0) get the feature matrix: normedTrain\n",
    "\n",
    "# 1) Subtract the mean from each column to get each feature to have zero mean. \n",
    "featuresDF = pd.DataFrame(normedTrain, dtype=float)\n",
    "for x in featuresDF.columns:\n",
    "    featuresDF[x] = featuresDF[x] - featuresDF[x].mean()\n",
    "\n",
    "# 2) Calculate the covariance matrix.\n",
    "X = featuresDF\n",
    "m = len(X)\n",
    "sigma = (1/m)*(X.T).dot(X)\n",
    "\n",
    "# 3) Perform SVD on sigma.\n",
    "U, S, Vtrans = np.linalg.svd(sigma, full_matrices=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2641, 2641)\n"
     ]
    }
   ],
   "source": [
    "# PCA continued from cell above.\n",
    "\n",
    "# 4) Grab the first k columns of U.\n",
    "U_oneHundred = U[:,0:100]\n",
    "U_fifty = U[:,0:50]\n",
    "U_ten = U[:,0:10]\n",
    "\n",
    "# 5) Take transposes of truncated U matrices\n",
    "U_oneHundredDF = pd.DataFrame(U_oneHundred, dtype=float)\n",
    "U_fiftyDF = pd.DataFrame(U_fifty, dtype=float)\n",
    "U_tenDF = pd.DataFrame(U_ten, dtype=float)\n",
    "\n",
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
