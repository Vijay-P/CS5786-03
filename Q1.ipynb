{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zack\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import nltk # import Natural Language Toolkit\n",
    "nltk.download('wordnet') # download the corpus of words the NLTK library uses\n",
    "from nltk.stem import WordNetLemmatizer # import the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input the file name as a string\n",
    "# Outputs two lists: [listOfReviews], [listOfLabels]\n",
    "def loadAndParse(inputFileName):\n",
    "    # Open file\n",
    "    fIn = open(inputFileName)\n",
    "\n",
    "    # split the the file into lines\n",
    "    lines = fIn.read().splitlines()\n",
    "\n",
    "    # Now split each line on tabs to get text and label\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in lines:\n",
    "        messageAndLabelList = review.split('\\t')\n",
    "        if(len(messageAndLabelList) != 2):\n",
    "            print(review)\n",
    "        message = messageAndLabelList[0]\n",
    "        label = messageAndLabelList[1]\n",
    "        reviews.append(message)\n",
    "        labels.append(label)\n",
    "    return reviews, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get three lists of reviews and three lists of labels\n",
    "yelpReviews, yelpLabels = loadAndParse('sentiment_labelled_sentences/yelp_labelled.txt')\n",
    "imdbReviews, imdbLabels = loadAndParse('sentiment_labelled_sentences/imdb_labelled.txt')\n",
    "amazonReviews, amazonLabels = loadAndParse('sentiment_labelled_sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "# Make two big lists: one of all reviews and one of all labels, in matching order.\n",
    "allReviews = []\n",
    "allLabels = []\n",
    "allReviews = yelpReviews + imdbReviews + amazonReviews\n",
    "allLabels = yelpLabels + imdbLabels + amazonLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON: There are 500 positive reviews.\n",
      "AMAZON: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "amazon_positives , amazon_negatives = 0, 0\n",
    "for label in amazonLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        amazon_positives += 1\n",
    "    if label == 0:\n",
    "        amazon_negatives += 1\n",
    "    \n",
    "print(\"AMAZON: There are\", amazon_positives, \"positive reviews.\")\n",
    "print(\"AMAZON: There are\", amazon_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB: There are 500 positive reviews.\n",
      "IMDB: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "imdb_positives , imdb_negatives = 0, 0\n",
    "for label in imdbLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        imdb_positives += 1\n",
    "    if label == 0:\n",
    "        imdb_negatives += 1\n",
    "    \n",
    "print(\"IMDB: There are\", imdb_positives, \"positive reviews.\")\n",
    "print(\"IMDB: There are\", imdb_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YELP: There are 500 positive reviews.\n",
      "YELP: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "yelp_positives , yelp_negatives = 0, 0\n",
    "for label in yelpLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        yelp_positives += 1\n",
    "    if label == 0:\n",
    "        yelp_negatives += 1\n",
    "    \n",
    "print(\"YELP: There are\", yelp_positives, \"positive reviews.\")\n",
    "print(\"YELP: There are\", yelp_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working punctuation remover. It can do whole sentences.\n",
    "def stripPunctuation(input):\n",
    "    translation_table = dict.fromkeys(map(ord, '\"\\'1234567890$#%&!()*+,-./:;<=>?@[\\]^_`{|}~'), None)\n",
    "    output = input.translate(translation_table)\n",
    "    # from: https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working word lemmatizer. It works on single words.\n",
    "def lemmatizeWord(input):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    output = lemmatize.lemmatize(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove punctuation and stop words from a sentence.\n",
    "def cleanAndRemoveStopWords(input):\n",
    "    stopWords = ['i', 'if', 'what','who','is','a','an','and', 'at','are','as','be','by','for', 'he', 'in', 'it', 'its', 'of', 'on', 'or', 'that', 'the', 'to', 'was', 'were', 'will', 'with']\n",
    "    queryWords = stripPunctuation(input).split()\n",
    "    resultWords = [lemmatizeWord(word) for word in queryWords if lemmatizeWord(word.lower()) not in stopWords]\n",
    "    \n",
    "    resultWordsLower = []\n",
    "    for word in resultWords:\n",
    "        resultWordsLower.append(word.lower())\n",
    "    \n",
    "    return resultWordsLower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.) Split the training set and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the training and test sets for each of the three websites\n",
    "amazonReviewsTrain = amazonReviews[0:400]\n",
    "amazonLabelsTrain = amazonLabels[0:400]\n",
    "amazonReviewsTest = amazonReviews[400: 500]\n",
    "amazonLabelsTest = amazonLabels[400:500]\n",
    "\n",
    "imdbReviewsTrain = imdbReviews[0:400]\n",
    "imdbLabelsTrain = imdbLabels[0:400]\n",
    "imdbReviewsTest = imdbReviews[400: 500]\n",
    "imdbLabelsTest = imdbLabels[400:500]\n",
    "\n",
    "yelpReviewsTrain = yelpReviews[0:400]\n",
    "yelpLabelsTrain = yelpLabels[0:400]\n",
    "yelpReviewsTest = yelpReviews[400: 500]\n",
    "yelpLabelsTest = yelpLabels[400:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d.) Bag of words model.\n",
    "Extract features and then represent each review using bag of words\n",
    "model, i.e., every word in the review becomes its own element in a feature vector. In order to\n",
    "do this, first, make one pass through all the reviews in the training set (**Explain why** we can’t\n",
    "use testing set at this point) and build a dictionary of unique words. Then,make another pass\n",
    "through the review in both the training set and testing set and count up the occurrences of\n",
    "each word in your dictionary. The i th element of a review’s feature vector is the number of\n",
    "occurrences of the i th dictionary word in the review. Implement the bag of words model and\n",
    "report feature vectors of any two reviews in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make lists of all training reviews and labels.\n",
    "allReviewsTrain = amazonReviewsTrain + imdbReviewsTrain + yelpReviewsTrain\n",
    "allLabelsTrain = amazonLabelsTrain + imdbLabelsTrain + yelpLabelsTrain\n",
    "\n",
    "allReviewsTest = amazonReviewsTest + imdbReviewsTest + yelpReviewsTest\n",
    "allLabelsTest = amazonLabelsTest + imdbLabelsTest + yelpLabelsTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a high level function that returns a non unique list of all non stop-words.\n",
    "# As input, it takes the test set or the training set of reviews.\n",
    "\n",
    "def cleanReviews(input):\n",
    "    result = []\n",
    "    for review in input:\n",
    "        cleanReview = cleanAndRemoveStopWords(review)\n",
    "        for cleanWord in cleanReview:\n",
    "            result.append(cleanWord)\n",
    "    return result\n",
    "\n",
    "allWordsNotUnique = cleanReviews(allReviewsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that makes a list of unique words (corpus), and a parallel list of counts \n",
    "# of each unique word. As input, it takes a list of non-unique words.\n",
    "def makeListsOfUniquesAndCounts(input):\n",
    "    uniques = []\n",
    "    for word in input:\n",
    "        if word not in uniques:\n",
    "            uniques.append(word)\n",
    "    # Sort the list of uniques\n",
    "    uniques.sort()\n",
    "    \n",
    "    #now \"uniques\" is an alphabetical list of all unique words in the corpus.\n",
    "    counts = [0]*len(uniques) # create a list of zeros as long as the number of unique words.\n",
    "    for word in input:\n",
    "        index = uniques.index(word)\n",
    "        counts[index] += 1 \n",
    "    return uniques, counts\n",
    "\n",
    "# Call the function to define the corpus.\n",
    "corpus, counts = makeListsOfUniquesAndCounts(allWordsNotUnique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test each review against the corpus (from the training set). This defines each feature vector.\n",
    "corpusSet = set(corpus)\n",
    "def populateFeatureVectors(corpus, inputReviewsList):\n",
    "    # feature vectors are row vectors, each column is a feature representing one word from the corpus.\n",
    "    vectorsList = []\n",
    "    for review in inputReviewsList:\n",
    "        cleanedReview = cleanAndRemoveStopWords(review)\n",
    "        featureVector = [0]*len(corpus)\n",
    "        for word in cleanedReview:\n",
    "            if word in corpusSet:\n",
    "                index = corpus.index(word)\n",
    "                featureVector[index] += 1 \n",
    "        vectorsList.append(featureVector)\n",
    "    return vectorsList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the training data.\n",
    "vectorsList = populateFeatureVectors(corpus, allReviewsTrain)\n",
    "# Make the list into a numpy array.\n",
    "vectorsArray = np.array(vectorsList)\n",
    "# Normalize each (now cleaned) row using the log norm, because it minimizes the variance of resulting vector.\n",
    "normedTrain = np.log10(vectorsArray + 1)\n",
    "\n",
    "\n",
    "# Clean the test data.\n",
    "cleanedTestReviews = populateFeatureVectors(corpus, allReviewsTest)\n",
    "# Make the list into a numpy array.\n",
    "testArray = np.array(cleanedTestReviews)\n",
    "# Norm the test data.\n",
    "normedTest = np.log10(testArray + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:      114       34\n",
      "Actual. F:       40      112\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T   Pred. F\n",
      "Actual. T:  0.380000  0.113333\n",
      "Actual. F:  0.133333  0.373333\n"
     ]
    }
   ],
   "source": [
    "# Use a logistic regression model to predict the labels of the list of test reviews.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model.\n",
    "sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_pred = sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain).predict(normedTest)\n",
    "# Print some results from the logistic regression to see how it does (it does very badly).\n",
    "    # print(\"Predicted labels: \", list(sk.linear_model.LogisticRegression().fit(normedTrain, allLabelsTrain).predict(normedTest[:15])) )\n",
    "    # print(\"Actual labels:    \", allLabels[:15])\n",
    "# Make a confusion matrix of the logistic regression results\n",
    "y_true = allLabelsTest\n",
    "logisticConfusionMtx = sk.metrics.confusion_matrix(y_true, y_pred)\n",
    "logisticConfusionMtxDF = pd.DataFrame(np.array(logisticConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(logisticConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAIVE BAYES\n",
      "\n",
      "Confusion Matrix\n",
      "            Pred. T  Pred. F\n",
      "Actual. T:       87       61\n",
      "Actual. F:       26      126\n",
      "\n",
      "Normalized Confusion Matrix\n",
      "             Pred. T   Pred. F\n",
      "Actual. T:  0.290000  0.203333\n",
      "Actual. F:  0.086667  0.420000\n"
     ]
    }
   ],
   "source": [
    "# Use Naive Bayes to predict the labels of the list of test reviews.\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train the model.\n",
    "y_predNB = GaussianNB().fit(normedTrain, allLabelsTrain)\n",
    "# Make predictions.\n",
    "y_predNB = GaussianNB().fit(normedTrain, allLabelsTrain).predict(normedTest)\n",
    "# Make a confusion matrix of the Naive Bayes prediction results\n",
    "y_true = allLabelsTest\n",
    "NBConfusionMtx = sk.metrics.confusion_matrix(y_true, y_predNB)\n",
    "NBConfusionMtxDF = pd.DataFrame(np.array(NBConfusionMtx), index=['Actual. T:', 'Actual. F:'], columns=['Pred. T', 'Pred. F'])\n",
    "\n",
    "print(\"NAIVE BAYES\")\n",
    "print()\n",
    "print(\"Confusion Matrix\")\n",
    "print(NBConfusionMtxDF)\n",
    "print()\n",
    "print(\"Normalized Confusion Matrix\")\n",
    "print(NBConfusionMtxDF/(len(y_true))) # Print the normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is this cell Needed???\n",
    "\n",
    "# # FOR TRAINING SET\n",
    "# # Construct parallel lists of unique words and counts\n",
    "# uniquesTrain, countsTrain = makeListsOfUniquesAndCounts(allWordsTrain)\n",
    "# # Construct dictionary of uniques and counts \n",
    "# trainingDataDict = makeDictOfUniquesAndCounts(uniquesTrain, countsTrain)\n",
    "\n",
    "# # FOR TEST+TRAIN COMBINED SET\n",
    "# # Construct parallel lists of unique words and counts\n",
    "# uniquesCombined, countsCombined = makeListsOfUniquesAndCounts(allWordsCombined)\n",
    "# # Construct dictionary of uniques and counts \n",
    "# # combinedDataDict = makeDictOfUniquesAndCounts(uniquesCombined, countsCombined)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Is the cell needed???\n",
    "\n",
    "# Build lists of words from training set\n",
    "# allWordsTrain = cleanReviews(allReviewsTrain)\n",
    "\n",
    "# allWordsCombined = cleanReviews(allReviewsTrain + allReviewsTest) DELETE this line??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Is this cell needed???\n",
    "\n",
    "# # Make a dictionary from a sorted list of unique words. \n",
    "# def makeDictOfUniquesAndCounts(uniques, counts):\n",
    "#     keys = uniques\n",
    "#     values = counts\n",
    "#     uniquesDict = {}\n",
    "#     for i in range(len(keys)):\n",
    "#         uniquesDict[keys[i]] = values[i] \n",
    "#     #print(uniquesDict)  \n",
    "#     return uniquesDict\n",
    "    \n",
    "# Call the function for testing.\n",
    "# makeDictOfUniquesAndCounts(uniques, counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
