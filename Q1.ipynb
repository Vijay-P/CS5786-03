{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/vijay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string \n",
    "import nltk # import Natural Language Toolkit\n",
    "nltk.download('wordnet') # download the corpus of words the NLTK library uses\n",
    "from nltk.stem import WordNetLemmatizer # import the lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input the file name as a string\n",
    "# Outputs two lists: [listOfReviews], [listOfLabels]\n",
    "def loadAndParse(inputFileName):\n",
    "    # Open file\n",
    "    fIn = open(inputFileName)\n",
    "\n",
    "    # split the the file into lines\n",
    "    lines = fIn.read().splitlines()\n",
    "\n",
    "    # Now split each line on tabs to get text and label\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in lines:\n",
    "        messageAndLabelList = review.split('\\t')\n",
    "        if(len(messageAndLabelList) != 2):\n",
    "            print(review)\n",
    "        message = messageAndLabelList[0]\n",
    "        label = messageAndLabelList[1]\n",
    "        reviews.append(message)\n",
    "        labels.append(label)\n",
    "    return reviews, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get three lists of reviews and three lists of labels\n",
    "yelpReviews, yelpLabels = loadAndParse('sentiment_labelled_sentences/yelp_labelled.txt')\n",
    "imdbReviews, imdbLabels = loadAndParse('sentiment_labelled_sentences/imdb_labelled.txt')\n",
    "amazonReviews, amazonLabels = loadAndParse('sentiment_labelled_sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "# Make two big lists: one of all reviews and one of all labels, in matching order.\n",
    "allReviews = []\n",
    "allLabels = []\n",
    "allReviews = yelpReviews + imdbReviews + amazonReviews\n",
    "allLabels = yelpLabels + imdbLabels + amazonLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMAZON: There are 500 positive reviews.\n",
      "AMAZON: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "amazon_positives , amazon_negatives = 0, 0\n",
    "for label in amazonLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        amazon_positives += 1\n",
    "    if label == 0:\n",
    "        amazon_negatives += 1\n",
    "    \n",
    "print(\"AMAZON: There are\", amazon_positives, \"positive reviews.\")\n",
    "print(\"AMAZON: There are\", amazon_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB: There are 500 positive reviews.\n",
      "IMDB: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "imdb_positives , imdb_negatives = 0, 0\n",
    "for label in imdbLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        imdb_positives += 1\n",
    "    if label == 0:\n",
    "        imdb_negatives += 1\n",
    "    \n",
    "print(\"IMDB: There are\", imdb_positives, \"positive reviews.\")\n",
    "print(\"IMDB: There are\", imdb_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YELP: There are 500 positive reviews.\n",
      "YELP: There are 500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "yelp_positives , yelp_negatives = 0, 0\n",
    "for label in yelpLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        yelp_positives += 1\n",
    "    if label == 0:\n",
    "        yelp_negatives += 1\n",
    "    \n",
    "print(\"YELP: There are\", yelp_positives, \"positive reviews.\")\n",
    "print(\"YELP: There are\", yelp_negatives, \"negative reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1500 positive reviews.\n",
      "There are 1500 negative reviews.\n"
     ]
    }
   ],
   "source": [
    "# Count the number of positive and negative reviews\n",
    "positives , negatives = 0, 0\n",
    "for label in allLabels:\n",
    "    label = int(label)\n",
    "    if label == 1:\n",
    "        positives += 1\n",
    "    if label == 0:\n",
    "        negatives += 1\n",
    "    \n",
    "print(\"There are\", positives, \"positive reviews.\")\n",
    "print(\"There are\", negatives, \"negative reviews.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working punctuation remover. It can do whole sentences.\n",
    "def stripPunctuation(input):\n",
    "    translation_table = dict.fromkeys(map(ord, '$#%&!()*+,-./:;<=>?@[\\]^_`{|}~'), None)\n",
    "    output = input.translate(translation_table)\n",
    "    # from: https://stackoverflow.com/questions/3939361/remove-specific-characters-from-a-string-in-python\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A working word lemmatizer. It works on single words.\n",
    "def lemmatizeWord(input):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    output = lemmatize.lemmatize(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanAndRemoveStopWords(input):\n",
    "    stopwords = ['what','who','is','a','at','is','he']\n",
    "    querywords = stripPunctuation(input).split()\n",
    "    resultwords  = [lemmatizeWord(word) for word in querywords if lemmatizeWord(word.lower()) not in stopwords]\n",
    "    result = ' '.join(resultwords)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEPS\n",
    "# 1) import each of the three files\n",
    "# 2) create a list of strings from each file, where each string is a message\n",
    "# 3) Clean each string: strip punctuation\n",
    "\n",
    "# 3b) Create training set and test set\n",
    "\n",
    "# 4) split each string into a list of individual words by splitting on spaces\n",
    "# 5) add the lists to make one big list of words \n",
    "# 6) lowercase and lemmatize every word in the list\n",
    "# 7) convert the list into a set to get rid of repeats. This set is the corpus.\n",
    "# 8) Maybe convert the set back into a list if that's needed to iterate over the list\n",
    "\n",
    "# ... remove stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
